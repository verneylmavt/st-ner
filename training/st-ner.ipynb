{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from seqeval.metrics import f1_score, classification_report, accuracy_score, recall_score, precision_score\n",
    "import optuna\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BiLSTMwAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seed Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "In here, the code sets the random seed for reproducibility across random, NumPy, and PyTorch operations. This ensures consistent results by fixing the seed for both CPU and GPU computations.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "In here, the code loads the CoNLL-2003 dataset using the load_dataset function with remote code execution trusted.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"conll2003\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tag Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "In here, the code extracts the set of unique NER tags from all dataset splits and creates mappings from tags to indices and vice versa.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_set = set()\n",
    "for split in ['train', 'validation', 'test']:\n",
    "    for tags in dataset[split]['ner_tags']:\n",
    "        tag_set.update(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_names = dataset['train'].features['ner_tags'].feature.names\n",
    "tagset_size = len(tag_set)\n",
    "\n",
    "tag2idx = {tag: idx for idx, tag in enumerate(tag_names)}\n",
    "idx2tag = {idx: tag for tag, idx in tag2idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Vocabulary Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "In here, the code counts the frequency of each word in the training set, selects the most common words up to a specified vocabulary size, and creates mappings from words to indices and vice versa, including special tokens for padding and unknown words.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counter = Counter()\n",
    "for tokens in dataset['train']['tokens']:\n",
    "    word_counter.update([token.lower() for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 100000\n",
    "special_tokens = ['<PAD>', '<UNK>']\n",
    "\n",
    "word2idx = {word: idx + len(special_tokens) for idx, (word, _) in enumerate(word_counter.most_common(vocab_size))}\n",
    "word2idx['<PAD>'] = 0\n",
    "word2idx['<UNK>'] = 1\n",
    "\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Character Vocabulary Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "In here, the code constructs a set of unique characters from all tokens across dataset splits and creates mappings from characters to indices and vice versa, including special tokens for padding and unknown characters.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_set = set()\n",
    "for split in ['train', 'validation', 'test']:\n",
    "    for tokens in dataset[split]['tokens']:\n",
    "        for token in tokens:\n",
    "            char_set.update(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx = {char: idx + 2 for idx, char in enumerate(sorted(char_set))}\n",
    "char2idx['<PAD>'] = 0\n",
    "char2idx['<UNK>'] = 1\n",
    "\n",
    "idx2char = {idx: char for char, idx in char2idx.items()}\n",
    "\n",
    "char_vocab_size = len(char2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "print(tagset_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "In here, the code defines a function to encode the dataset by converting words and characters to their respective indices, handling padding for both words and characters, and applies this encoding to the training, validation, and test sets.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 50\n",
    "max_char_len = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data_with_chars(split):\n",
    "    data = []\n",
    "    for tokens, tags in zip(dataset[split]['tokens'], dataset[split]['ner_tags']):\n",
    "        word_ids = [word2idx.get(token.lower(), word2idx['<UNK>']) for token in tokens]\n",
    "        tag_ids = [tag for tag in tags]\n",
    "        char_ids = []\n",
    "        for token in tokens:\n",
    "            chars = [char2idx.get(c, char2idx['<UNK>']) for c in token]\n",
    "            if len(chars) > max_char_len:\n",
    "                chars = chars[:max_char_len]\n",
    "            else:\n",
    "                chars += [char2idx['<PAD>']] * (max_char_len - len(chars))\n",
    "            char_ids.append(chars)\n",
    "        if len(word_ids) > max_len:\n",
    "            word_ids = word_ids[:max_len]\n",
    "            tag_ids = tag_ids[:max_len]\n",
    "            char_ids = char_ids[:max_len]\n",
    "        else:\n",
    "            pad_length = max_len - len(word_ids)\n",
    "            word_ids += [word2idx['<PAD>']] * pad_length\n",
    "            tag_ids += [-100] * pad_length\n",
    "            char_ids += [[char2idx['<PAD>']] * max_char_len] * pad_length\n",
    "        data.append((word_ids, char_ids, tag_ids))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = encode_data_with_chars('train')\n",
    "val_data = encode_data_with_chars('validation')\n",
    "test_data = encode_data_with_chars('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "In here, the code loads pre-trained GloVe embeddings from a specified file path, initializes an embedding matrix with these vectors for known words, and assigns random vectors to words not present in the GloVe embeddings.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 200\n",
    "embeddings_index = {}\n",
    "glove_file_path = \"../../data/glove.6B.200d/glove.6B.200d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400000/400000 [00:11<00:00, 35116.33it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(glove_file_path, encoding='utf-8') as f:\n",
    "    for line in tqdm(f, total=400000):\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word2idx), embedding_dim))\n",
    "\n",
    "for word, idx in word2idx.items():\n",
    "    if word in embeddings_index:\n",
    "        embedding_matrix[idx] = embeddings_index[word]\n",
    "    else:\n",
    "        embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
    "        \n",
    "embedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "In here, the code defines a custom dataset class that handles word and character indices along with their corresponding tags. It then creates dataset instances for training, validation, and testing, and wraps them in DataLoader objects for batching.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERDatasetWithChars(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        word_ids, char_ids, tag_ids = self.data[idx]\n",
    "        return {\n",
    "            'words': torch.tensor(word_ids, dtype=torch.long),\n",
    "            'chars': torch.tensor(char_ids, dtype=torch.long),\n",
    "            'tags': torch.tensor(tag_ids, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NERDatasetWithChars(train_data)\n",
    "val_dataset = NERDatasetWithChars(val_data)\n",
    "test_dataset = NERDatasetWithChars(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_iter = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_iter = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_iter = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "In here, the code defines a BiLSTM model with character embeddings and an attention mechanism for Named Entity Recognition. The model includes embedding layers for words and characters, LSTM layers for processing sequences, a multi-head attention layer, and a fully connected layer for tag prediction.\n",
    "\n",
    "• Word Embeddings\n",
    "The word embedding layer transforms input words into dense vector representations of fixed size. This layer is initialized with a pre-trained embedding matrix if provided, allowing the model to benefit from semantic information learned from large external corpora. Dropout regularization is applied to word embeddings to prevent overfitting and improve generalization.\n",
    "\n",
    "• Character Embeddings\n",
    "To capture subword-level information, the model includes a character embedding layer. Each character in a word is embedded into a dense vector. A bidirectional LSTM processes these character embeddings, producing a combined forward and backward hidden state for each word. This character-level feature is concatenated with the corresponding word embedding, enabling the model to better handle out-of-vocabulary words and learn morphological patterns.\n",
    "\n",
    "• BiLSTM\n",
    "The concatenated word and character embeddings are fed into a bidirectional LSTM. This layer captures contextual information by processing sequences in both forward and backward directions. The hidden states from the BiLSTM encode the contextual dependencies of words within the input sequence. A dropout layer and layer normalization are applied to the BiLSTM output to stabilize training and reduce overfitting.\n",
    "\n",
    "• Multi-Head Attention\n",
    "The multi-head attention layer operates on the BiLSTM outputs, allowing the model to attend to different parts of the sequence when producing a representation for each word. This mechanism enhances the model’s ability to capture long-range dependencies and focus on important tokens. The attention output is combined with the BiLSTM output via a residual connection, further improving representation quality and preserving the original sequential information.\n",
    "\n",
    "• Fully Connected Layer\n",
    "The combined output from the residual connection is passed through a fully connected layer, mapping it to the tag space. This final layer is responsible for producing the tag predictions for each token in the sequence, such as entity labels for NER tasks.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMwA(nn.Module):\n",
    "    def __init__(self, vocab_size, char_vocab_size, word_embedding_dim, char_embedding_dim,\n",
    "                 char_hidden_dim, hidden_dim, tagset_size, embeddings=None, dropout=0.5):\n",
    "        super(BiLSTMwA, self).__init__()\n",
    "        \n",
    "        # Embedding Layer for Word w/ Padding Index for '<PAD>'\n",
    "        self.word_embedding = nn.Embedding(vocab_size, word_embedding_dim, padding_idx=word2idx['<PAD>'])\n",
    "        # Parameter Layer for Word Embeddings Initialization w/ Pre-Trained Embeddings (if provided)\n",
    "        if embeddings is not None:\n",
    "            self.word_embedding.weight = nn.Parameter(embeddings)\n",
    "            self.word_embedding.weight.requires_grad = True # Gradient Enabling for Fine-Tuning\n",
    "        # Dropout Layer for Word Regularization\n",
    "        self.word_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Embedding Layer for Character w/ Padding Index for '<PAD>'\n",
    "        self.char_embedding = nn.Embedding(char_vocab_size, char_embedding_dim, padding_idx=char2idx['<PAD>'])\n",
    "        # UniLSTM Layer for Character Feature Extraction\n",
    "        self.char_lstm = nn.LSTM(char_embedding_dim, char_hidden_dim, num_layers=1,\n",
    "                                 bidirectional=True, batch_first=True)\n",
    "        # Dropout Layer for Character Regularization\n",
    "        self.char_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # BiLSTM Layer for Contextual Word and Character Representation\n",
    "        self.bilstm = nn.LSTM(word_embedding_dim + 2 * char_hidden_dim, hidden_dim // 2,\n",
    "                              num_layers=2, bidirectional=True, batch_first=True, dropout=dropout)\n",
    "        # Normalization Layer for Word and Character Stabilization\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        # Dropout Layer for Word and Character Regularization\n",
    "        self.lstm_dropout = nn.Dropout(dropout)\n",
    "        # Multi-Head Attention Layer for Contextual Word and Character Feature Extraction\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=8, dropout=dropout, batch_first=True)\n",
    "        # Fully Connected Layer for Word and Character → Tag Space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "    \n",
    "    \n",
    "    def forward(self, words, chars):\n",
    "        # Word Embeddings of Input Words\n",
    "        word_embeds = self.word_embedding(words)\n",
    "        # Dropout of Word Embeddings\n",
    "        word_embeds = self.word_dropout(word_embeds)\n",
    "\n",
    "        batch_size, seq_len, char_len = chars.size()\n",
    "        chars = chars.view(batch_size * seq_len, char_len)\n",
    "        # Character Embeddings of Input Characters\n",
    "        char_embeds = self.char_embedding(chars)\n",
    "        # UniLSTM of Character Embeddings \n",
    "        char_lstm_out, _ = self.char_lstm(char_embeds)\n",
    "        # Concatenation of Forward and Backward Character Hidden States of Last Time Step\n",
    "        char_hidden = torch.cat((char_lstm_out[:, -1, :char_lstm_out.size(2)//2],\n",
    "                                 char_lstm_out[:, -1, char_lstm_out.size(2)//2:]), dim=1)\n",
    "        # Dropout of Character Hidden States\n",
    "        char_hidden = self.char_dropout(char_hidden)\n",
    "        # Reshaping of Character Hidden States → Word Embeddings\n",
    "        char_hidden = char_hidden.view(batch_size, seq_len, -1)\n",
    "        \n",
    "        # Concatenation of Word Embeddings and Character Hidden States\n",
    "        combined = torch.cat((word_embeds, char_hidden), dim=2)\n",
    "        # BiLSTM of Word Embeddings and Character Hidden States\n",
    "        lstm_out, _ = self.bilstm(combined)\n",
    "        # Normalization of BiLSTM Output\n",
    "        lstm_out = self.layer_norm(lstm_out)\n",
    "        # Dropout of BiLSTM Output\n",
    "        lstm_out = self.lstm_dropout(lstm_out)\n",
    "        # Attention of BiLSTM Output\n",
    "        attn_output, attn_weights = self.attention(lstm_out, lstm_out, lstm_out)\n",
    "        # Residual Connection: BiLSTM Output + Attention Output\n",
    "        combined_output = lstm_out + attn_output\n",
    "        # Transformation of Residual Connection → Tag Space\n",
    "        tag_space = self.hidden2tag(combined_output)\n",
    "        return tag_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "In here, the code defines a function to train the model for one epoch. It processes each batch, computes the loss, performs backpropagation, updates the model parameters, and calculates the training accuracy.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(net, train_iter, criterion, optimizer, device):\n",
    "    net.train()\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "\n",
    "    for batch in tqdm(train_iter, desc=\"Training\", leave=False):\n",
    "        words = batch['words'].to(device)\n",
    "        chars = batch['chars'].to(device)\n",
    "        tags = batch['tags'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        emissions = net(words, chars)\n",
    "\n",
    "        loss = criterion(emissions.view(-1, emissions.size(-1)), tags.view(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(net.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, preds = torch.max(emissions, dim=2)\n",
    "        acc = (preds == tags).float().sum() / (tags != -100).sum()\n",
    "        total_acc += acc.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_iter)\n",
    "    avg_acc = total_acc / len(train_iter)\n",
    "    return avg_loss, avg_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "In here, the code defines a function to evaluate the model on a validation set. It computes the loss and accuracy without updating the model parameters and collects all predictions and true labels for further metric calculations.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_epoch(net, val_iter, criterion, device):\n",
    "    net.eval()\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_iter, desc=\"Evaluating\", leave=False):\n",
    "            words = batch['words'].to(device)\n",
    "            chars = batch['chars'].to(device)\n",
    "            tags = batch['tags'].to(device)\n",
    "\n",
    "            emissions = net(words, chars)\n",
    "\n",
    "            loss = criterion(emissions.view(-1, emissions.size(-1)), tags.view(-1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, preds = torch.max(emissions, dim=2)\n",
    "            acc = (preds == tags).float().sum() / (tags != -100).sum()\n",
    "            total_acc += acc.item()\n",
    "\n",
    "            preds = preds.cpu().numpy()\n",
    "            tags = tags.cpu().numpy()\n",
    "\n",
    "            for pred_seq, tag_seq in zip(preds, tags):\n",
    "                pred_tags = []\n",
    "                true_tags = []\n",
    "                for p, t in zip(pred_seq, tag_seq):\n",
    "                    if t != -100:\n",
    "                        pred_tags.append(idx2tag[p])\n",
    "                        true_tags.append(idx2tag[t])\n",
    "                all_preds.append(pred_tags)\n",
    "                all_labels.append(true_tags)\n",
    "\n",
    "    avg_loss = total_loss / len(val_iter)\n",
    "    avg_acc = total_acc / len(val_iter)\n",
    "    return avg_loss, avg_acc, all_preds, all_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "In here, the code utilizes Optuna to perform hyperparameter tuning. It defines an objective function that suggests hyperparameters, initializes the model with these parameters, trains the model for a set number of epochs with early stopping based on validation F1 score, and returns the best F1 score achieved. The study is then optimized over a specified number of trials to find the best hyperparameters.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    lr = trial.suggest_loguniform('lr', 1e-5, 1e-2)\n",
    "    hidden_dim = trial.suggest_int('hidden_dim', 256, 1024, step=128)\n",
    "    dropout = trial.suggest_float('dropout', 0.3, 0.7, step=0.1)\n",
    "\n",
    "    net = BiLSTMwA(\n",
    "        vocab_size=vocab_size,\n",
    "        char_vocab_size=char_vocab_size,\n",
    "        word_embedding_dim=embedding_dim,\n",
    "        char_embedding_dim=50,\n",
    "        char_hidden_dim=50,\n",
    "        hidden_dim=hidden_dim,\n",
    "        tagset_size=tagset_size,\n",
    "        embeddings=embedding_matrix,\n",
    "        dropout=dropout\n",
    "    )\n",
    "    net.to(device)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "    best_val_f1 = 0.0\n",
    "    counter = 0\n",
    "    patience = 2\n",
    "    num_epochs_tuning = 5\n",
    "\n",
    "    for epoch in range(num_epochs_tuning):\n",
    "        train_loss, train_acc = train_epoch(net, train_iter, criterion, optimizer, device)\n",
    "        val_loss, val_acc, all_preds, all_labels = evaluate_epoch(net, val_iter, criterion, device)\n",
    "        val_f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            torch.save(net.state_dict(), \"best_model.pth\")\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                break\n",
    "    return best_val_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# study = optuna.create_study(direction='maximize', study_name='NER_Hyperparam_Tuning')\n",
    "# study.optimize(objective, n_trials=20)\n",
    "# best_trial = study.best_trial    \n",
    "# best_params = best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"\\nBest Hyperparameters:\")\n",
    "# for key, value in best_params.items():\n",
    "#     print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "In here, the code initializes the BiLSTM model with the best hyperparameters obtained from tuning, sets up the optimizer and loss function, and trains the model for a specified number of epochs with early stopping based on validation F1 score. It also records training and validation metrics for visualization and saves the best-performing model.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTMwA(\n",
       "  (word_embedding): Embedding(100000, 200, padding_idx=0)\n",
       "  (word_dropout): Dropout(p=0.4, inplace=False)\n",
       "  (char_embedding): Embedding(87, 50, padding_idx=0)\n",
       "  (char_lstm): LSTM(50, 50, batch_first=True, bidirectional=True)\n",
       "  (char_dropout): Dropout(p=0.4, inplace=False)\n",
       "  (bilstm): LSTM(300, 128, num_layers=2, batch_first=True, dropout=0.4, bidirectional=True)\n",
       "  (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  (lstm_dropout): Dropout(p=0.4, inplace=False)\n",
       "  (attention): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (hidden2tag): Linear(in_features=256, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = BiLSTMwA(\n",
    "    vocab_size=vocab_size,\n",
    "    char_vocab_size=char_vocab_size,\n",
    "    word_embedding_dim=embedding_dim,\n",
    "    char_embedding_dim=50,\n",
    "    # hidden_dim=best_params['hidden_dim'],\n",
    "    hidden_dim=256,\n",
    "    char_hidden_dim=50,\n",
    "    tagset_size=tagset_size,\n",
    "    embeddings=embedding_matrix,\n",
    "    # dropout=best_params['dropout']\n",
    "    dropout=0.4\n",
    ")\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "# optimizer = optim.AdamW(net.parameters(), lr=best_params['lr'], weight_decay=0.01)\n",
    "optimizer = optim.AdamW(net.parameters(), lr=0.002071759505536834, weight_decay=0.01)\n",
    "num_epochs = 15\n",
    "num_epochs_used = 0\n",
    "\n",
    "best_val_f1 = 0.0\n",
    "counter = 0\n",
    "patience = 3\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.2450, train acc: 0.9299, val acc: 0.9754\n",
      "\n",
      "epoch 2/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.0729, train acc: 0.9789, val acc: 0.9793\n",
      "\n",
      "epoch 3/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.0474, train acc: 0.9864, val acc: 0.9804\n",
      "\n",
      "epoch 4/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.0354, train acc: 0.9898, val acc: 0.9826\n",
      "\n",
      "epoch 5/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.0250, train acc: 0.9926, val acc: 0.9827\n",
      "\n",
      "epoch 6/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.0192, train acc: 0.9943, val acc: 0.9836\n",
      "\n",
      "epoch 7/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.0157, train acc: 0.9953, val acc: 0.9827\n",
      "\n",
      "epoch 8/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.0134, train acc: 0.9961, val acc: 0.9772\n",
      "\n",
      "epoch 9/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.0107, train acc: 0.9969, val acc: 0.9781\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    num_epochs_used += 1\n",
    "    print(f\"\\nepoch {epoch + 1}/{num_epochs}\")\n",
    "    train_loss, train_acc = train_epoch(net, train_iter, criterion, optimizer, device)\n",
    "    val_loss, val_acc, all_preds, all_labels = evaluate_epoch(net, val_iter, criterion, device)\n",
    "    val_f1 = f1_score(all_labels, all_preds)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_accuracies.append(val_acc)\n",
    "    \n",
    "    print(f\"train loss: {train_loss:.4f}, train acc: {train_acc:.4f}, val acc: {val_acc:.4f}\")\n",
    "    \n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        torch.save(net.state_dict(), \"best_model.pth\")\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs_range = range(1, num_epochs_used + 1)\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "plt.plot(epochs_range, train_losses, label='train loss', linestyle='-', color='#2a7db8')\n",
    "plt.plot(epochs_range, train_accuracies, label='train acc', linestyle='--', color='magenta')\n",
    "plt.plot(epochs_range, val_accuracies, label='val acc', linestyle='-.', color='green')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('value')\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "In here, the code defines a function to calculate and display evaluation metrics such as accuracy, precision, recall, F1 score, and a detailed classification report on the test set. It then invokes this function to evaluate the trained model.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_2432\\1080333206.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(torch.load(\"best_model.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.load_state_dict(torch.load(\"best_model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_metrics(net, test_iter):\n",
    "    net.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_iter, desc=\"Testing\"):\n",
    "            words = batch['words'].to(device)\n",
    "            chars = batch['chars'].to(device)\n",
    "            tags = batch['tags'].to(device)\n",
    "            \n",
    "            emissions = net(words, chars)\n",
    "            _, preds = torch.max(emissions, dim=2)\n",
    "            \n",
    "            preds = preds.cpu().numpy()\n",
    "            tags = tags.cpu().numpy()\n",
    "            \n",
    "            for pred_seq, tag_seq in zip(preds, tags):\n",
    "                pred_tags = []\n",
    "                true_tags = []\n",
    "                for p, t in zip(pred_seq, tag_seq):\n",
    "                    if t != -100:\n",
    "                        pred_tags.append(idx2tag[p])\n",
    "                        true_tags.append(idx2tag[t])\n",
    "                all_preds.append(pred_tags)\n",
    "                all_labels.append(true_tags)\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    report = classification_report(all_labels, all_preds)\n",
    "\n",
    "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1 Score:  {f1:.4f}\")\n",
    "    print(\"\\nClassification Report:\\n\", report)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 54/54 [00:00<00:00, 66.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9682\n",
      "Precision: 0.8105\n",
      "Recall:    0.8433\n",
      "F1 Score:  0.8266\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.91      0.85      0.88      1656\n",
      "        MISC       0.75      0.73      0.74       701\n",
      "         ORG       0.75      0.79      0.77      1658\n",
      "         PER       0.81      0.94      0.87      1580\n",
      "\n",
      "   micro avg       0.81      0.84      0.83      5595\n",
      "   macro avg       0.80      0.83      0.81      5595\n",
      "weighted avg       0.81      0.84      0.83      5595\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cal_metrics(net, test_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "In here, the code defines a function to predict NER tags for a given input sentence. It tokenizes the sentence, converts tokens and characters to their respective indices, feeds them into the model to obtain predictions, and then maps the predicted indices back to their corresponding NER tags. Finally, it prints out each token with its predicted tag.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ner(net, sentence, word2idx, char2idx, idx2tag):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    device = torch.device('cpu')\n",
    "    net.to(device)\n",
    "    net.eval()\n",
    "    max_len = 50\n",
    "    max_char_len = 10\n",
    "    word_ids = []\n",
    "    char_ids = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        word_id = word2idx.get(token.lower(), word2idx['<UNK>'])\n",
    "        word_ids.append(word_id)\n",
    "        chars_of_token = [char2idx.get(c, char2idx['<UNK>']) for c in token]\n",
    "        if len(chars_of_token) > max_char_len:\n",
    "            chars_of_token = chars_of_token[:max_char_len]\n",
    "        else:\n",
    "            chars_of_token += [char2idx['<PAD>']] * (max_char_len - len(chars_of_token))\n",
    "        char_ids.append(chars_of_token)\n",
    "    \n",
    "    if len(word_ids) > max_len:\n",
    "        word_ids = word_ids[:max_len]\n",
    "        char_ids = char_ids[:max_len]\n",
    "    else:\n",
    "        pad_length = max_len - len(word_ids)\n",
    "        word_ids += [word2idx['<PAD>']] * pad_length\n",
    "        char_ids += [[char2idx['<PAD>']] * max_char_len] * pad_length\n",
    "    \n",
    "    word_tensor = torch.tensor([word_ids], dtype=torch.long, device=device)\n",
    "    char_tensor = torch.tensor([char_ids], dtype=torch.long, device=device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        emissions = net(word_tensor, char_tensor)\n",
    "        _, preds = torch.max(emissions, dim=2)\n",
    "    preds = preds.squeeze(0).cpu().numpy()\n",
    "    real_length = min(len(tokens), max_len)\n",
    "    pred_tags = [idx2tag[preds[i]] for i in range(real_length)]\n",
    "    \n",
    "    return tokens, pred_tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John B-PER\n",
      "lives O\n",
      "in O\n",
      "New B-LOC\n",
      "York I-LOC\n"
     ]
    }
   ],
   "source": [
    "tokens, predicted_tags = predict_ner(net, \"John lives in New York\", word2idx, char2idx, idx2tag)\n",
    "for token, tag in zip(tokens, predicted_tags):\n",
    "    print(token, tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ONNX Exporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def net_to_onnx(net, word2idx, char2idx, onnx_file_path=\"model.onnx\"):\n",
    "#     device = torch.device('cpu')\n",
    "#     net.to(device)\n",
    "#     net.eval()\n",
    "#     max_len = 50\n",
    "#     max_char_len = 10\n",
    "#     word_ids = [word2idx['<PAD>']] * max_len\n",
    "#     char_ids = [[char2idx['<PAD>']] * max_char_len] * max_len\n",
    "#     word_tensor = torch.tensor([word_ids], dtype=torch.long, device=device)\n",
    "#     char_tensor = torch.tensor([char_ids], dtype=torch.long, device=device)\n",
    "    \n",
    "#     torch.onnx.export(\n",
    "#         net, \n",
    "#         (word_tensor, char_tensor),\n",
    "#         onnx_file_path, \n",
    "#         export_params=True,\n",
    "#         opset_version=13,\n",
    "#         input_names=[\"word_ids\", \"char_ids\"],\n",
    "#         output_names=[\"emissions\"],\n",
    "#         dynamic_axes={\n",
    "#             \"word_ids\": {0: \"batch_size\", 1: \"seq_length\"},\n",
    "#             \"char_ids\": {0: \"batch_size\", 1: \"seq_length\", 2: \"char_length\"},\n",
    "#             \"emissions\": {0: \"batch_size\", 1: \"seq_length\"}\n",
    "#         }\n",
    "#     )\n",
    "#     return None\n",
    "\n",
    "# net_to_onnx(net, word2idx, char2idx, \"model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "# def quantize_onnx(input_model_path, output_model_path):\n",
    "#     quantize_dynamic(\n",
    "#         model_input=input_model_path,\n",
    "#         model_output=output_model_path,\n",
    "#         weight_type=QuantType.QInt8\n",
    "#     )\n",
    "#     return None\n",
    "\n",
    "# original_model_path = \"model.onnx\"\n",
    "# quantized_model_path = \"model-q.onnx\"\n",
    "# quantize_onnx(original_model_path, quantized_model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
